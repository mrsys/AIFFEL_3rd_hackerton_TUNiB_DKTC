{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "utilities",
      "provenance": [],
      "authorship_tag": "ABX9TyNKwrrrabBTf8hNSZiSh3gd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sda96/AIFFEL_3rd_hackerton_TUNiB_DKTC/blob/main/notebook/SeHyun/Custom%20Libraries/utilities.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGNZah0USQMI"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_and_perplexity_for_language_model(data_list1, data_list2, epoch_list, figure_size=(7,1), dpi_value=300, figure_name=''):\n",
        "    \"\"\"\n",
        "    :param data_list1: loss list(dtype: list)\n",
        "    :param data_list2: perplexity list(dtype: list)\n",
        "    :param epoch_list: epoch list(dtype: list)\n",
        "    :param figure_name: Name of the figure to be saved(dtype: string)\n",
        "    :return: Plots the Loss and perplexity of the language model.\n",
        "    \"\"\"\n",
        "    fig1 = plt.figure(figsize=figure_size, dpi=dpi_value)\n",
        "    plt.plot(epoch_list, data_list1, 'b', label='val_loss')\n",
        "    plt.plot(epoch_list, data_list2, 'r', label='perplexity')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss and Perplexity\")\n",
        "    plt.title(\"Loss-Perplexity curve for \" + figure_name + \" data\" )\n",
        "    fig1.savefig(figure_name + \"_loss_curve.png\", bbox_inches='tight')\n",
        "\n",
        "\n",
        "def batchify(data, bsz, device):\n",
        "    \"\"\"\n",
        "    :param data: data corpus(could be train, test, or validation dataset)\n",
        "    :param bsz: Batch size(dtype: int32)\n",
        "    :param device: GPU/CPU(dtype: torch.device)\n",
        "    :return: dataset divided into batches\n",
        "    \"\"\"\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "EkPa9JcXSUiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"\n",
        "    :param h: hidden state(dtype: torch.tensor)\n",
        "    :return: Wraps hidden states in new Tensors, to detach them from their history.\n",
        "    \"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "\n",
        "\n",
        "def make_batches(data, bptt, i):\n",
        "    \"\"\"\n",
        "    :param data: data corpus(could be train, test, or validation dataset)\n",
        "    :param bptt: Backpropogation through time or sequence length(dtype: int32)\n",
        "    :param i: Iterated chunks(dtype: int32)\n",
        "    :return: subdivides the input data into chunks of length bptt and generates source and targets for model to train\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(data) - 1 - i)\n",
        "    inputs = data[i:i+seq_len]\n",
        "    targets = data[i+1:i+1+seq_len].view(-1)\n",
        "    return inputs, targets\n",
        "\n"
      ],
      "metadata": {
        "id": "gCylEB3jSWqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_sent_tsv_line_sup(sents, tagger, tag_type):            \n",
        "    \"\"\"\n",
        "    :param sents: sentences in paragraphs(dtype:list of strings)\n",
        "    :param tagger: POS/NER tagger from flair/nltk\n",
        "    :param tag_type: tag type('pos'/'ner')\n",
        "    :return: array of tagged sentences with their associated 'pos'/'ner' for supervised corpus\n",
        "    \"\"\"\n",
        "    tagger.predict(sents)\n",
        "    tags = []\n",
        "    for s in sents:                                        # tag a batch of sentence and pipe out tsv lines\n",
        "        temp_tags = [str(t.get_tag(tag_type)) for t in s]       # throws error for wrong tag type\n",
        "        tags.append([re.sub(r'\\([^)]*\\)', '', tag) for tag in temp_tags])\n",
        "    return tags[0]\n",
        "\n",
        "def tag_sent_tsv_line_unsup(sents, tagger, tag_type):            \n",
        "    \"\"\"\n",
        "    :param sents: sentences in paragraphs(dtype:list of strings)\n",
        "    :param tagger: POS/NER tagger from flair/nltk\n",
        "    :param tag_type: tag type('pos'/'ner')\n",
        "    :return: array of tagged sentences with their associated 'pos'/'ner' for unsupervised corpus\n",
        "    \"\"\"\n",
        "    tagger.predict(sents)\n",
        "    tags = []\n",
        "    for s in sents:                                        # tag a batch of sentence and pipe out tsv lines\n",
        "        temp_tags = [str(t.get_tag(tag_type)) for t in s]       # throws error for wrong tag type\n",
        "        tags.append([re.sub(r'\\([^)]*\\)', '', tag) for tag in temp_tags])\n",
        "    return tags\n",
        "\n",
        "\n",
        "def sent_2_index(seq, to_ix, cuda=False):\n",
        "    \"\"\"\n",
        "    :param seq: sequence list with pararaphs denoted by list of tokens(dtype:list).\n",
        "    :param to_ix: word to index mappings(dtype:dict)\n",
        "    :return: Long tensor for all the tokens converted to their respective ids\n",
        "    \"\"\"\n",
        "    var = autograd.Variable(torch.LongTensor([to_ix[w.lower()] if w.lower() in to_ix.keys() else to_ix[\"unk\"] for w in seq]))\n",
        "    return var\n",
        "\n",
        "def label_2_index(label, label_to_ix, cuda=False):\n",
        "    \"\"\"\n",
        "    :param label: sequence list of labels(dtype:list).\n",
        "    :param label_to_ix: labels to index mappings(dtype:dict)\n",
        "    :return: Long tensor for all the labels converted to their respective ids(negative being zero and positive being one)\n",
        "    \"\"\" \n",
        "    var = autograd.Variable(torch.LongTensor([label_to_ix[label]]))\n",
        "    return var\n",
        "\n",
        "def evaluate_supervised_model(model, data, loss_function, word_to_ix, label_to_ix, data_acc_list, data_roc_list,\n",
        "                             data_loss_list, name ='valid'):\n",
        "    \"\"\"\n",
        "    :param model: trained model\n",
        "    :param data: data to evaluated on(dtype: pandas.core.frame.DataFrame)\n",
        "    :param loss_function: loss function used for evaluation\n",
        "    :param word_to_ix: word to index mappings(dtype: dict)\n",
        "    :param label_to_ix: label to index mappings(dtype: dict)\n",
        "    :param data_acc_list: a list to collect accuracy at every epoch(dtype: list)\n",
        "    :param data_roc_list: a list to collect roc score at every epoch(dtype: list)\n",
        "    :param data_loss_list: a list to collect loss at every epoch(dtype: list)\n",
        "    :param name: type of data(Could be 'train','test',or 'valid'. dtype: string)\n",
        "    :return: evaluated accuracy and roc on the entire dataset, data_acc_list, data_roc_list, data_loss_list\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    avg_loss = 0.0\n",
        "    truth_res, pred_res = ([] for i in range(2))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for (para, label) in zip(data[\"review\"],data[\"label\"]):\n",
        "            truth_res.append(label_to_ix[label])\n",
        "            para = sent_2_index(para, word_to_ix)\n",
        "            label = label_2_index(label, label_to_ix)\n",
        "            y_pred, value_dict = model(para, True)\n",
        "            # The predicted results are processed through BCElosswithlogits, hence the outputs are\n",
        "            # passed through the sigmoid layer to turn it into probabilities. \n",
        "            pred_res.append(float(torch.sigmoid(torch.FloatTensor(y_pred))))\n",
        "            # Since the loss_function already has a sigmoid layer attached to it, we don't need to pass the predictions\n",
        "            # again through another sigmoid layer.\n",
        "            loss = loss_function(y_pred, label.type(torch.FloatTensor))\n",
        "            avg_loss += loss.item()\n",
        "    \n",
        "    avg_loss /= len(data)\n",
        "    roc = roc_auc_score(np.array(truth_res), np.array(pred_res).round(), sample_weight=None)\n",
        "    pred_res = [0 if values > 0.5 else 1 for values in pred_res]\n",
        "    acc = accuracy_score(np.array(truth_res), np.array(pred_res).round(), sample_weight=None)\n",
        "    \n",
        "    data_roc_list.append(roc)\n",
        "    data_loss_list.append(avg_loss)\n",
        "    data_acc_list.append(acc)\n",
        "    \n",
        "    print(' '*16 + name + ':|avg_loss:%g|ROC:%g|Accuracy:%g|' % (avg_loss, roc, acc))\n",
        "    return acc, roc, data_acc_list, data_roc_list, data_loss_list\n"
      ],
      "metadata": {
        "id": "BY1_7LgtSWsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_supervised_model(model, train_data, loss_function, optimizer, word_to_ix, label_to_ix, i, train_acc_list, \n",
        "                           train_roc_list, train_loss_list, batch_size=32, clip=5):\n",
        "    \"\"\"\n",
        "    :param model: the model to be trained\n",
        "    :param train_data: Training data(dtype: pandas.core.frame.DataFrame)\n",
        "    :param loss_function: loss function used for evaluation\n",
        "    :param optimizer: Optimizer used while training\n",
        "    :param word_to_ix: word to index mappings(dtype: dict)\n",
        "    :param label_to_ix: label to index mappings(dtype: dict)\n",
        "    :param i: number of steps passed(dtype: int)\n",
        "    :param train_acc_list: a list to collect accuracy at every epoch(dtype: list)\n",
        "    :param train_roc_list: a list to collect roc score at every epoch(dtype: list)\n",
        "    :param train_loss_list: a list to collect loss at every epoch(dtype: list)\n",
        "    :param batch_size: batch size(dtype: int)\n",
        "    :param clip: clip rate(dtype: int)\n",
        "    :return: train_acc_list, train_roc_list, train_loss_list\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    truth_res, pred_res = ([] for i in range(2))\n",
        "    avg_loss = 0.0\n",
        "    count = 0\n",
        "    \n",
        "    for (para, label) in zip(train_data[\"review\"],train_data[\"label\"]):\n",
        "        truth_res.append(label_to_ix[label])        \n",
        "        para = sent_2_index(para, word_to_ix)\n",
        "        label = label_2_index(label, label_to_ix)\n",
        "        y_pred, value_dict = model(para)\n",
        "        # The predicted results are processed through BCElosswithlogits, hence the outputs are\n",
        "        # passed through the sigmoid layer to turn it into probabilities. \n",
        "        pred_res.append(float(torch.sigmoid(torch.FloatTensor(y_pred))))\n",
        "        # Since the loss_function already has a sigmoid layer attached to it, we don't need to pass the predictions\n",
        "        # again through another sigmoid layer.\n",
        "        loss = loss_function(y_pred, label.type(torch.FloatTensor))\n",
        "        loss.backward()\n",
        "        current_loss = loss.item()\n",
        "        avg_loss += current_loss\n",
        "        \n",
        "        count += 1\n",
        "        if count % 10000 == 0:\n",
        "            print('|paragraphs: %d|loss :%g|' % (count, current_loss))\n",
        "        \n",
        "        if count % batch_size == 0:\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            #raise Exception(\"Arrange clip as per your batch size\")\n",
        "            #raise Exception(\"Try with and without clipping\")\n",
        "            if clip != None:\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            \n",
        "    avg_loss /= len(train_data)\n",
        "    print('-' * 100)\n",
        "    \n",
        "    train_loss_list.append(avg_loss)\n",
        "    roc = roc_auc_score(np.array(truth_res), np.array(pred_res).round(), sample_weight=None)\n",
        "    pred_res = [0 if values > 0.5 else 1 for values in pred_res]\n",
        "    acc = accuracy_score(np.array(truth_res), np.array(pred_res).round(), sample_weight=None)\n",
        "    \n",
        "    train_roc_list.append(roc)\n",
        "    train_acc_list.append(acc)\n",
        "    \n",
        "    print('|End of Epoch:%d|Training data:|avg_loss:%g|ROC:%g|Accuracy:%g|'%(int(i+1), avg_loss, roc, acc))\n",
        "    return train_acc_list, train_roc_list, train_loss_list\n"
      ],
      "metadata": {
        "id": "a4y0a1RBSem0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_and_generate_plots(model, train_data, test_data, val_data, unsup_word_to_idx, label_to_idx, learning_rate, batch_size, \n",
        "                                   nb_epochs, save_dir, description, early_stopping=5):\n",
        "    \"\"\"\n",
        "    :param model: the model to be trained\n",
        "    :param train_data: Training data(dtype: pandas.core.frame.DataFrame)\n",
        "    :param test_data: Test data(dtype: pandas.core.frame.DataFrame)\n",
        "    :param val_data: Validation data(dtype: pandas.core.frame.DataFrame)\n",
        "    :param unsup_word_to_idx: word to index mappings(dtype: dict)\n",
        "    :param label_to_idx: label to index mappings(dtype: dict)\n",
        "    :param learning_rate: Learning rate(dtype:float)\n",
        "    :param nb_epochs: number of Epochs(dtype:int)\n",
        "    :param save_dir: directory for the model to be saved(dtype:string)\n",
        "    :param batch_size: Batch size(dtype:int)\n",
        "    :param early_stopping: After how many steps should the model stop training if the val_roc doesn't change(dtype:int)\n",
        "    :param description: Data desciption(Train,test, or validation; dtype:string)\n",
        "    \"\"\"\n",
        "    loss_function = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    optimizer.zero_grad()\n",
        "    train_acc_list, train_roc_list, train_loss_list, test_acc_list, test_roc_list, test_loss_list, val_acc_list, val_roc_list, val_loss_list, epoch_list = ([] for i in range(10))\n",
        "    \n",
        "    #optimizer = torch.optim.SGD(model.parameters(), lr = 1e-2)\n",
        "    no_up = 0\n",
        "    best_val_roc = 0.0\n",
        "    \n",
        "    for i in range(nb_epochs):\n",
        "        epoch_list.append(int(i+1))\n",
        "        print('epoch: %d start!' % int(i+1))\n",
        "        \n",
        "        # Training the model\n",
        "        optimizer.zero_grad()\n",
        "        # pbar = tqdm(total=train_data.shape[0])\n",
        "        train_acc_list, train_roc_list, train_loss_list = train_supervised_model(model, train_data, loss_function, optimizer, unsup_word_to_idx, label_to_idx, i, train_acc_list, train_roc_list, train_loss_list, batch_size)\n",
        "        \n",
        "        # Hyper-tuning the model\n",
        "        optimizer.zero_grad()\n",
        "        val_acc, val_roc, val_acc_list, val_roc_list, val_loss_list = evaluate_supervised_model(model, val_data, loss_function, unsup_word_to_idx, label_to_idx, val_acc_list, val_roc_list, val_loss_list, 'validation data')\n",
        "        \n",
        "        # Testing the model\n",
        "        optimizer.zero_grad()\n",
        "        test_acc, test_roc, test_acc_list, test_roc_list, test_loss_list = evaluate_supervised_model(model,test_data, loss_function, unsup_word_to_idx, label_to_idx, test_acc_list, test_roc_list, test_loss_list, 'test data')\n",
        "        \n",
        "        # Un-comment the below lines if you want to save models with smallest change in val_acc and val_roc\n",
        "        \"\"\"if (val_acc > best_val_acc) and (val_roc <= best_val_roc):\n",
        "            # Saving models on the basis of validation accuracy\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_models_lstm_1500/acc_' + str(int(test_acc*100))\n",
        "                       + \"_roc_\" + str(int(test_roc*100)) + '.pt')\n",
        "            no_up = 0\n",
        "        elif (val_roc > best_val_roc) and (val_acc <= best_val_acc):\n",
        "            # Saving models on the basis of validation roc\n",
        "            best_val_roc = val_roc\n",
        "            torch.save(model.state_dict(), 'best_models_lstm_1500/roc_' + str(int(test_roc*100))\n",
        "                       + \"_acc_\" + str(int(test_acc*100)) + '.pt')\n",
        "            no_up = 0\n",
        "        elif (val_roc > best_val_roc) and (val_acc > best_val_acc):\n",
        "            # Saving models on the basis of validation roc and validation accuracy\n",
        "            best_val_roc = val_roc\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_models_lstm_1500/combined_roc_' + str(int(test_roc*100))\n",
        "                       + \"_acc_\" + str(int(test_acc*100)) + '.pt')\n",
        "            no_up = 0\"\"\"\n",
        "\n",
        "        if val_roc > best_val_roc:\n",
        "            torch.save(model.state_dict(), save_dir)\n",
        "            best_val_acc = val_roc\n",
        "\n",
        "        else:\n",
        "            # early stopping\n",
        "            no_up += 1\n",
        "            if no_up >= 5:\n",
        "                break\n",
        "    \n",
        "    # Un-comment the below lines to generate training, test, and validation plots\n",
        "    \"\"\"\n",
        "    # Saving the lists in a dataframe so that it can be used to plot the variations wrt epochs.\n",
        "    df = pd.DataFrame({\"epochs\":epoch_list, \"train_acc\": train_acc_list, \"train_roc\": train_roc_list,\n",
        "                       \"train_loss\":train_loss_list, \"val_acc\" : val_acc_list, \"val_roc\": val_roc_list, \"val_loss\" :\n",
        "                       val_loss_list, \"test_acc\" : test_acc_list, \"test_roc\": test_roc_list, \"test_loss\" : test_loss_list})\n",
        "    plot = df.plot(x=\"epochs\",y=[\"train_acc\",\"test_acc\",\"val_acc\"],title= \"Accuracy curve\")\n",
        "    fig = plot.get_figure()\n",
        "    fig.savefig(description + \"_acc.png\")\n",
        "        \n",
        "    plot = df.plot(x=\"epochs\",y=[\"train_loss\",\"test_loss\",\"val_loss\"],title=\"Loss curve\")\n",
        "    fig = plot.get_figure()\n",
        "    fig.savefig(description + \"_loss.png\")\n",
        "    \n",
        "    plot = df.plot(x=\"epochs\",y=[\"train_roc\",\"test_roc\",\"val_roc\"],title=\"ROC curve\")\n",
        "    fig = plot.get_figure()\n",
        "    fig.savefig(description + \"_roc.png\")\n",
        "    \"\"\"\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "gtLcHh3PSepc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}