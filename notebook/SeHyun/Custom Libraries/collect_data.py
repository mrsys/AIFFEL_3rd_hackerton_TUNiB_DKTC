{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "collect_data",
      "provenance": [],
      "authorship_tag": "ABX9TyNIXPOvrInq0q1SOeT+Hlxm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sda96/AIFFEL_3rd_hackerton_TUNiB_DKTC/blob/main/notebook/SeHyun/Custom%20Libraries/collect_data.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRNNcroMQ9ti"
      },
      "outputs": [],
      "source": [
        "#Importing libraries\n",
        "\n",
        "import os\n",
        "import csv\n",
        "from io import open\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import flair\n",
        "from flair.models import SequenceTagger\n",
        "from flair.data import Sentence\n",
        "from textblob import TextBlob\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Custom libraries\n",
        "from load_data import Corpus\n",
        "from layers import LSTMLM, LSTMClassifier, LSTMPoolingClassifier\n",
        "from utilities import tag_sent_tsv_line_unsup, tag_sent_tsv_line_sup, sent_2_index, label_2_index\n",
        "from preprocess import DataPreprocessor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAggregrator:\n",
        "    def __init__(self, trained_model, data_dict, corpus, id_to_word, word_to_id, label_to_ix, mode=None):\n",
        "        self.trained_model = trained_model\n",
        "        self.data_dict = data_dict\n",
        "        self.corpus = corpus\n",
        "        self.id_to_word = id_to_word\n",
        "        self.word_to_id = word_to_id\n",
        "        self.label_to_ix = label_to_ix\n",
        "        self.mode = mode\n",
        "\n",
        "    def prepare_unsup_data_for_saving(self, para, tagger):\n",
        "        para.insert(len(para), \"eop\")\n",
        "        para = \" \".join(para)\n",
        "        t = TextBlob(para)\n",
        "        sents = []\n",
        "        for sent in t.sentences:\n",
        "            sents.append(Sentence(sent.strip()))\n",
        "        # Generating POS tags tagged to the sentence\n",
        "        pos = tag_sent_tsv_line_unsup(sents, tagger, self.data_dict['tagger_type'])\n",
        "        pos = np.array([item for sublist in pos for item in sublist])\n",
        "        para = word_tokenize(para)\n",
        "        para = [torch.tensor(self.word_to_id[w]) if w in self.word_to_id.keys() else torch.tensor(self.word_to_id['unk']) for w in para]\n",
        "        return para, pos\n",
        "\n",
        "    def save_unsup_data(self, para, pos, hidden, tsv_writer):\n",
        "        if len(para) == len(pos):\n",
        "            for index,token in enumerate(para):\n",
        "                inputs = token\n",
        "                encoded_outputs, decoded_outputs, hidden = self.trained_model(inputs.unsqueeze(0).unsqueeze(0), hidden)\n",
        "                word_weights = torch.nn.functional.softmax(Variable(decoded_outputs.squeeze(0).squeeze(0))).data\n",
        "                input_word = self.id_to_word[inputs]\n",
        "                output_word = self.id_to_word[word_weights.argmax()]\n",
        "                pos_tag = pos[index]\n",
        "                activation = list(hidden[0].squeeze(0).squeeze(0).data.numpy())\n",
        "                tsv_writer.writerow([input_word, output_word, activation, pos_tag, np.max(activation), np.argmax(activation)])\n",
        "                # hidden_activation_dict.setdefault(output_word, []).append(hidden[0].squeeze(0).squeeze(0).data.numpy())\n",
        "                # Resetting the hidden layer to zero at the end of every sentence.\n",
        "                if input_word == \"eop\":\n",
        "                    hidden = self.trained_model.init_hidden(1)\n",
        "\n",
        "    def save_sup_data(self, inputs_words, tagger, value_dict, tsv_writer):\n",
        "        # For nltk.pos_tag to work, we have to use word_tokenize. While using word_tokenizer it is splitting\n",
        "        # \"<\" and \">\" into individual tokens and thereby its necessary to remove them.\n",
        "        inputs_sentence = \" \".join(inputs_words)\n",
        "        t = TextBlob(inputs_sentence)\n",
        "        sents = []\n",
        "        for sent in t.sentences:\n",
        "            sents.append(Sentence(sent.strip()))\n",
        "        pos = tag_sent_tsv_line_sup(sents, tagger, self.data_dict['tagger_type'])    \n",
        "        value_dict['POS'] = pos\n",
        "\n",
        "        if len(value_dict['inputs']) == len(value_dict['POS']):\n",
        "            tsv_writer.writerows(zip(*[value_dict[key] for key in value_dict.keys()]))\n",
        "\n",
        "    def record_sup_data(self, truth_res, label, para):\n",
        "        truth_res.append(self.label_to_ix[label])\n",
        "        para = sent_2_index(para, self.word_to_id)\n",
        "        label = label_2_index(label, self.label_to_ix)\n",
        "        y_pred, value_dict = self.trained_model(para, True)\n",
        "        # The predicted results are processed through BCElosswithlogits,c hence the outputs are\n",
        "        # passed through the sigmoid layer to turn it into probabilities. \n",
        "        y_pred = float(torch.sigmoid(torch.FloatTensor(y_pred)))\n",
        "        y_pred = [y_pred] * len(value_dict['activations'])\n",
        "        value_dict[\"prediction_score\"] = y_pred\n",
        "\n",
        "        actual_label = [int(label)] * len(value_dict['activations'])\n",
        "        value_dict[\"labels\"] = actual_label\n",
        "\n",
        "        # Changing the shape and format of data to be written in tsv file\n",
        "        act_temp = []\n",
        "        for activation in value_dict[\"activations\"]:\n",
        "            act = [float(value) for value in activation.squeeze_()]\n",
        "            act_temp.append(act)\n",
        "        value_dict['activations'] = act_temp\n",
        "\n",
        "        inputs_words = []\n",
        "        for token_id in value_dict[\"inputs\"]:\n",
        "            inputs_words.append(self.id_to_word[token_id[0]])\n",
        "        value_dict['inputs'] = np.array(inputs_words)\n",
        "        return inputs_words, value_dict\n",
        "\n",
        "    def unsup_data(self):\n",
        "        \"\"\"\n",
        "        :return: Aggregrates unsupervised data for lstm language model in a .tsv file\n",
        "        \"\"\"\n",
        "        # Pre-processing data\n",
        "        process_data = DataPreprocessor(self.corpus, self.id_to_word, self.word_to_id)\n",
        "        para_list, id_to_word, word_to_id = process_data.unsup_model()\n",
        "\n",
        "        # Generating data and saving it in a .tsv file\n",
        "        self.trained_model.eval()\n",
        "        hidden = self.trained_model.init_hidden(1)\n",
        "        tagger = SequenceTagger.load(self.data_dict['tagger_type'])\n",
        "        \n",
        "        if self.mode == None:\n",
        "            filename = self.data_dict[\"models\"][\"pretrained_lm\"][\"saved_tsv_dir\"]\n",
        "        elif self.mode == \"1\":\n",
        "            filename = os.path.join(\"data\",\"unsup_data1.tsv\")\n",
        "        elif self.mode == \"48\":\n",
        "            filename = os.path.join(\"data\",\"unsup_data48.tsv\")\n",
        "        else:\n",
        "            filename = os.path.join(\"data\",\"unsup_data49.tsv\")\n",
        "\n",
        "        with open(filename, 'wt') as out_file:\n",
        "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            tsv_writer.writerow(self.data_dict[\"models\"][\"pretrained_lm\"][\"fields_to_generate\"])\n",
        "            for para in para_list:\n",
        "                para, pos = self.prepare_unsup_data_for_saving(para, tagger)\n",
        "                self.save_unsup_data(para, pos, hidden, tsv_writer)\n",
        "\n",
        "\n",
        "    def sup_data(self):\n",
        "        \"\"\"\n",
        "        :return: Aggregrates supervised data for lstm classifier in a .tsv file\n",
        "        \"\"\"\n",
        "        self.trained_model.eval()\n",
        "        truth_res, pred_res = ([] for i in range(2))\n",
        "        tagger = SequenceTagger.load(self.data_dict['tagger_type'])\n",
        "\n",
        "        with open(self.data_dict[\"models\"][\"lstm_classifier\"][\"saved_tsv_dir\"], 'wt') as out_file:\n",
        "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            tsv_writer.writerow(self.data_dict[\"models\"][\"lstm_classifier\"][\"fields_to_generate\"])\n",
        "            \n",
        "            for (para, label) in zip(self.corpus[\"review\"],self.corpus[\"label\"]):\n",
        "                inputs_words, value_dict = self.record_sup_data(truth_res, label, para)\n",
        "                self.save_sup_data(inputs_words, tagger, value_dict, tsv_writer)\n",
        "\n",
        "    def sup_pooled_data(self):\n",
        "        \"\"\"\n",
        "        :return: Aggregrates supervised data for lstm classifier in a .tsv file\n",
        "        \"\"\"\n",
        "        self.trained_model.eval()\n",
        "        truth_res, pred_res = ([] for i in range(2))\n",
        "        tagger = SequenceTagger.load(self.data_dict['tagger_type'])\n",
        "\n",
        "        with open(self.data_dict[\"models\"][\"lstm_pooled_classifier\"][\"saved_tsv_dir\"], 'wt') as out_file:\n",
        "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            tsv_writer.writerow(self.data_dict[\"models\"][\"lstm_pooled_classifier\"][\"fields_to_generate\"])\n",
        "            \n",
        "            for (para, label) in zip(self.corpus[\"review\"],self.corpus[\"label\"]):\n",
        "                inputs_words, value_dict = self.record_sup_data(truth_res, label, para)\n",
        "                self.save_sup_data(inputs_words, tagger, value_dict, tsv_writer)\n",
        "\n",
        "    def save_zero_shot_data(self, pos, para, hidden, tsv_writer):\n",
        "        if len(pos) == len(para):\n",
        "            for index,tokens in enumerate(para):\n",
        "                inputs = tokens\n",
        "                encoded_outputs, decoded_outputs, hidden = self.trained_model(inputs.unsqueeze(0).unsqueeze(0), hidden)\n",
        "                word_weights = torch.nn.functional.softmax(Variable(decoded_outputs.squeeze(0).squeeze(0))).data\n",
        "                input_word = self.id_to_word[inputs]\n",
        "                output_word = self.id_to_word[word_weights.argmax()]\n",
        "                pos_tag = pos[index]\n",
        "                activation = list(hidden[0].squeeze(0).squeeze(0).data.numpy())\n",
        "                tsv_writer.writerow([input_word, output_word, activation, pos_tag, np.max(activation), np.argmax(activation)])\n",
        "                # hidden_activation_dict.setdefault(output_word, []).append(hidden[0].squeeze(0).squeeze(0).data.numpy())\n",
        "                # Resetting the hidden layer to zero at the end of every sentence.\n",
        "                if input_word == \"eos\":\n",
        "                    hidden = self.trained_model.init_hidden(1)\n",
        "\n",
        "    def prepare_zero_shot_data(self, para, tagger):\n",
        "        pos = []\n",
        "        # para = [self.id_to_word[token] for token in para]\n",
        "        para = \" \".join(para)\n",
        "        t = TextBlob(para)\n",
        "        sents = []\n",
        "        for sent in t.sentences:\n",
        "            sents.append(Sentence(sent.strip()))\n",
        "        pos = tag_sent_tsv_line_unsup(sents, tagger, self.data_dict['tagger_type'])\n",
        "        pos = np.array([item for sublist in pos for item in sublist])\n",
        "\n",
        "        para = word_tokenize(para)\n",
        "        para = [torch.tensor(self.word_to_id[w]) if w in self.word_to_id.keys() else torch.tensor(self.word_to_id['unk']) for w in para]\n",
        "        return pos, para\n",
        "\n",
        "    def zero_shot_data(self):\n",
        "        \"\"\"\n",
        "        :return: Aggregrates zero shot data for unsupervised classifier in a .tsv file\n",
        "        \"\"\"\n",
        "        self.trained_model.eval()\n",
        "        hidden = self.trained_model.init_hidden(1)\n",
        "        tagger = SequenceTagger.load(self.data_dict['tagger_type'])\n",
        "\n",
        "        with open(self.data_dict[\"models\"][\"zero_shot\"][\"saved_tsv_dir\"], 'w') as out_file:\n",
        "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
        "            tsv_writer.writerow(self.data_dict[\"models\"][\"zero_shot\"][\"fields_to_generate\"])\n",
        "\n",
        "            for para in list(self.corpus['review']):\n",
        "                # converting paragraphs into tokens\n",
        "                pos, para = self.prepare_zero_shot_data(para, tagger)\n",
        "                self.save_zero_shot_data(pos, para, hidden, tsv_writer)"
      ],
      "metadata": {
        "id": "-ZNRDr19REmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Kka501ywREop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dzJhDNuDREq9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}