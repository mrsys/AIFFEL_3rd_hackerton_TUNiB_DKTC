{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "plots",
      "provenance": [],
      "authorship_tag": "ABX9TyPRXPUKWFJwCjliTEBeb6+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sda96/AIFFEL_3rd_hackerton_TUNiB_DKTC/blob/main/notebook/SeHyun/Custom%20Libraries/plots.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXPoQgYBOifM"
      },
      "outputs": [],
      "source": [
        "##import libraries\n",
        "import os\n",
        "import sys\n",
        "import heapq\n",
        "import operator\n",
        "import pickle\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Custom Libraries\n",
        "sys.path.append('../../Evaluation_metrics/')\n",
        "from measures import find_shared_neurons\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mass_activation_plot(unsup_data, zero_shot_data, sup_data, data_dict):\n",
        "    \"\"\"\n",
        "    :param unsup_data: Unsupervised data(dtype:pandas dataframe)\n",
        "    :param zero_shot_data: Zero shot data(dtype:pandas dataframe)\n",
        "    :param sup_data: Supervised data(dtype:pandas dataframe)\n",
        "    :param data_dict: dictionary containing input instructions(dtype:dict)\n",
        "    Plots the mass activation plot and save it in data_dict[\"visualize\"][\"plot_directory\"]\n",
        "    \"\"\"\n",
        "    \n",
        "    if not os.path.exists(data_dict[\"visualize\"][\"plot_directory\"]):\n",
        "        os.makedirs(data_dict[\"visualize\"][\"plot_directory\"])\n",
        "        \n",
        "    zero_shot_neurons = list(zero_shot_data['max_activation_index'].unique())\n",
        "    unsup_neurons = list(unsup_data['max_activation_index'].unique())\n",
        "    sup_neurons = list(sup_data['max_activation_index'].unique())\n",
        "    \n",
        "    zero_shot_mass_dict, unsup_mass_dict, sup_mass_dict = ({} for i in range(3))\n",
        "    \n",
        "    for neuron in unsup_neurons:\n",
        "        temp = unsup_data[unsup_data['max_activation_index']==neuron]\n",
        "        unsup_mass_dict[neuron] = sum(temp['max_activations'])\n",
        "    for neuron in zero_shot_neurons:\n",
        "        temp = zero_shot_data[zero_shot_data['max_activation_index']==neuron]\n",
        "        zero_shot_mass_dict[neuron] = sum(temp['max_activations'])\n",
        "    for neuron in sup_neurons:\n",
        "        temp = sup_data[sup_data['max_activation_index']==neuron]\n",
        "        sup_mass_dict[neuron] = sum(temp['max_activations'])\n",
        "        \n",
        "    sup = [value[1] for value in sorted(sup_mass_dict.items(), key=operator.itemgetter(1), reverse=True)]\n",
        "    unsup = [value[1] for value in sorted(unsup_mass_dict.items(), key=operator.itemgetter(1), reverse=True)]\n",
        "    zshot = [value[1] for value in sorted(zero_shot_mass_dict.items(), key=operator.itemgetter(1), reverse=True)] \n",
        "    \n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Bar(y=sup, name=\"sup\", marker_color=data_dict['visualize']['viz_colors']['sup_color']))\n",
        "    fig.add_trace(go.Bar(y=unsup, name=\"unsup\", marker_color=data_dict['visualize']['viz_colors']['unsup_color']))\n",
        "    fig.add_trace(go.Bar(y=zshot, name=\"zshot\", marker_color=data_dict['visualize']['viz_colors']['zero_shot_color']))\n",
        "    \n",
        "    fig.update_layout(barmode='relative', \n",
        "                    title_text='Mass activations for neurons',\n",
        "                    xaxis_title=\"Neurons\",\n",
        "                    yaxis_title=\"Log mass Activations\",\n",
        "                    yaxis_type=\"log\",\n",
        "                    xaxis = go.XAxis(showticklabels=False),\n",
        "                    yaxis = go.YAxis(showticklabels=False)\n",
        "                    )\n",
        "\n",
        "    # fig.write_image(os.path.join(data_dict[\"visualize\"][\"plot_directory\"], \"mass_activation_plot.pdf\"))\n",
        "    plotly.offline.plot(fig, filename = os.path.join(data_dict[\"visualize\"][\"plot_directory\"], \"mass_activation_plot.pdf\"),\n",
        "                        auto_open=False)\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "inh7Lj4vOzKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def freq_analysis_plot(sup_data, unsup_data_epoch1, unsup_data_epoch49, data_dict):\n",
        "    \"\"\"\n",
        "    :param unsup_data_epoch1: Unsupervised data for 1 epoch(dtype:pandas dataframe)\n",
        "    :param unsup_data_epoch49: Unsupervised data for 1 epoch(dtype:pandas dataframe)\n",
        "    :param sup_data: Supervised data(dtype:pandas dataframe)\n",
        "    :param data_dict: dictionary containing input instructions(dtype:dict)\n",
        "    Plots the frequency analysis plot and save it in data_dict[\"visualize\"][\"plot_directory\"]\n",
        "    \"\"\"\n",
        "    \n",
        "    sup = sup_data['POS'].to_dict()\n",
        "    sup_freq = Counter(sup.values())\n",
        "    sup_freq = dict(OrderedDict(sup_freq.most_common()))\n",
        "\n",
        "    unsup1 = unsup_data_epoch1['POS'].to_dict()\n",
        "    unsup1_freq = Counter(unsup1.values())\n",
        "    unsup1_freq = dict(OrderedDict(unsup1_freq.most_common()))\n",
        "\n",
        "    unsup49 = unsup_data_epoch49['POS'].to_dict()\n",
        "    unsup49_freq = Counter(unsup49.values())\n",
        "    unsup49_freq = dict(OrderedDict(unsup49_freq.most_common()))\n",
        "\n",
        "    unsup1_dataframe = pd.DataFrame.from_dict(unsup1_freq, orient='index', columns=['unsup-1'])\n",
        "    unsup49_dataframe = pd.DataFrame.from_dict(unsup49_freq, orient='index', columns=['unsup-49'])\n",
        "    sup_dataframe = pd.DataFrame.from_dict(sup_freq, orient='index', columns=['sup'])\n",
        "\n",
        "    unsup1_pos = list(unsup1_dataframe.index)\n",
        "    unsup1_pos_mass_activation = []\n",
        "    for pos in unsup1_pos:\n",
        "        temp = unsup_data_epoch1[unsup_data_epoch1['POS']==pos]\n",
        "        unsup1_pos_mass_activation.append(temp['max_activations'].sum())\n",
        "\n",
        "    sup_pos = list(sup_dataframe.index)\n",
        "    sup_pos_mass_activation = []\n",
        "    for pos in sup_pos:\n",
        "        temp = sup_data[sup_data['POS']==pos]\n",
        "        sup_pos_mass_activation.append(temp['max_activations'].sum())\n",
        "\n",
        "    unsup49_pos = list(unsup49_dataframe.index)\n",
        "    unsup49_pos_mass_activation = []\n",
        "    for pos in unsup49_pos:\n",
        "        temp = unsup_data_epoch49[unsup_data_epoch49['POS']==pos]\n",
        "        unsup49_pos_mass_activation.append(temp['max_activations'].sum())\n",
        "\n",
        "    unsup1_dataframe['unsup1-mass_activation'] = unsup1_pos_mass_activation\n",
        "    unsup49_dataframe['unsup49-mass_activation'] = unsup49_pos_mass_activation\n",
        "    sup_dataframe['sup-mass_activation'] = sup_pos_mass_activation\n",
        "\n",
        "    df = unsup1_dataframe.join(sup_dataframe)\n",
        "    df_ = df.join(unsup49_dataframe)\n",
        "\n",
        "    df_.sort_values(['unsup-1'],inplace=True,ascending=False)\n",
        "    df_['unsup corpus POS freq. %'] = df_['unsup-1'].apply(lambda x:x/df_['unsup-1'].sum())\n",
        "    df_['unsup epoch 1 act. mass %'] = df_['unsup1-mass_activation'].apply(lambda x:x/df_['unsup1-mass_activation'].sum())\n",
        "    df_['unsup epoch 49 act. mass %'] = df_['unsup49-mass_activation'].apply(lambda x:x/df_['unsup49-mass_activation'].sum())\n",
        "\n",
        "    plot_dict = df_[['unsup corpus POS freq. %','unsup epoch 1 act. mass %','unsup epoch 49 act. mass %']].to_dict()\n",
        "\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Bar(x= list(plot_dict['unsup corpus POS freq. %'].keys()) ,\n",
        "                         y= list(plot_dict['unsup corpus POS freq. %'].values()), \n",
        "                         name=\"unsup POS freq. %\", marker_color='black'))\n",
        "    fig.add_trace(go.Bar(x= list(plot_dict['unsup epoch 1 act. mass %'].keys()) ,\n",
        "                         y= list(plot_dict['unsup epoch 1 act. mass %'].values()), \n",
        "                         name=\"unsup epoch 1 act. mass %\", marker_color='gray'))\n",
        "    fig.add_trace(go.Bar(x= list(plot_dict['unsup epoch 49 act. mass %'].keys()) ,\n",
        "                         y= list(plot_dict['unsup epoch 49 act. mass %'].values()), \n",
        "                         name=\"unsup epoch 49 act. mass %\", marker_color=data_dict['visualize']['viz_colors']['unsup_epoch_49']))\n",
        "\n",
        "    fig.update_layout(barmode='relative', \n",
        "                        title_text='% POS activations vs. % POS frequencies',\n",
        "                        xaxis_title=\"POS tags\",\n",
        "                        yaxis_title=\"POS %\",\n",
        "                        )\n",
        "\n",
        "    # fig.write_image(os.path.join(data_dict[\"visualize\"][\"plot_directory\"], \"mass_activation_plot.pdf\"))\n",
        "    plotly.offline.plot(fig, filename = os.path.join(data_dict[\"visualize\"][\"plot_directory\"], \"freq_activation_plot.pdf\"),\n",
        "                        auto_open=False)\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "VnHy4RV0OzMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hellinger_length_plot(hellinger_stats, filename):\n",
        "    \"\"\"\n",
        "    :param hellinger_stats: path to the savd file for the hellinger statistics from calculate_hellinger_distance function \n",
        "    :param filename: file name with directory where the results are to be stored(dtype:str)\n",
        "    Description: Plots a scatter plot between number of features activated for every neuron vs hellinger distance between \n",
        "                the two models\n",
        "    \"\"\"\n",
        "\n",
        "    with open(hellinger_stats, 'rb') as handle:\n",
        "        hellinger_dict = pickle.load(handle)\n",
        "\n",
        "    num_token_list, distance_list = ([] for i in range(2))\n",
        "    for activation,(distance,num_tokens) in hellinger_dict.items():   \n",
        "        num_token_list.append(num_tokens)\n",
        "        distance_list.append(distance)\n",
        "\n",
        "    fig = px.scatter(x= num_token_list ,y= distance_list)\n",
        "    \n",
        "    plot_title = str(len(hellinger_dict)) + \" neurons activated\"\n",
        "    fig.update_layout(barmode='relative', \n",
        "                        title_text=plot_title,\n",
        "                        xaxis_title=\"Log Hellinger length\",\n",
        "                        yaxis_title=\"Hellinger distance\",\n",
        "                        xaxis_type=\"log\",\n",
        "                        xaxis = go.XAxis(showticklabels=False),\n",
        "                        yaxis = go.YAxis(showticklabels=False)\n",
        "                        )\n",
        "    \n",
        "    plotly.offline.plot(fig, filename = filename,auto_open=False)\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "Mx57LgcOOzQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def length_shift_token_plot(model1, model2, modelname1, modelname2, color1, color2, y_axis_label_model1, \n",
        "                            y_axis_label_model2, data_dict, filename):\n",
        "    \"\"\"\n",
        "    :param model1:data from trained model 1(dtype:dataframe)\n",
        "    :param model2:data from trained model 2(dtype:dataframe)\n",
        "    :param color1:color for model 1(dtype:str)\n",
        "    :param color2:color for model 2(dtype:str)\n",
        "    :param modelname1:model1 label(dtype:str)\n",
        "    :param modelname2:model2 label(dtype:str)\n",
        "    :param y_axis_label_model1:Y axis label annotation for model1(dtype:str)\n",
        "    :param y_axis_label_model2:Y axis label annotation for model2(dtype:str)\n",
        "    :param data_dict: dictionary containing input instructions(dtype:dict)\n",
        "    :param filename: pickled file name and directory to store the results\n",
        "    \"\"\"\n",
        "        \n",
        "    fig = go.Figure()\n",
        "    # Collecting number of tokens in each neurons for both the model\n",
        "    model1_tokens_dict, model2_tokens_dict = ({} for i in range(2))\n",
        "    for neuron in list(model1['max_activation_index'].unique()):\n",
        "        model1_data = model1[model1['max_activation_index'] == neuron]\n",
        "        model1_tokens_dict[neuron] = model1_data['inputs'].nunique()\n",
        "    for neuron in list(model2['max_activation_index'].unique()):\n",
        "        model2_data = model2[model2['max_activation_index'] == neuron]\n",
        "        model2_tokens_dict[neuron] = model2_data['inputs'].nunique()\n",
        "\n",
        "    model1_token_list, model1_y_list = ([] for i in range(2))\n",
        "    model2_token_list, model2_y_list = ([] for i in range(2))\n",
        "    # plotting scatter plot\n",
        "    for neuron in range(data_dict['models']['pretrained_lm']['nhid']):\n",
        "        if neuron in list(model1['max_activation_index'].unique()):\n",
        "            model1_token_list.append(model1_tokens_dict[neuron])\n",
        "            model1_y_list.append(y_axis_label_model1)\n",
        "            \n",
        "        if neuron in list(model2['max_activation_index'].unique()):\n",
        "            model2_token_list.append(model2_tokens_dict[neuron])\n",
        "            model2_y_list.append(y_axis_label_model2)\n",
        "    \n",
        "    fig.add_trace(go.Scatter(x=model1_token_list,y= model1_y_list, mode='markers', name=modelname1 ,\n",
        "                             marker_color=color1))\n",
        "    fig.add_trace(go.Scatter(x= model2_token_list, y=model2_y_list, mode='markers', name=modelname2,\n",
        "                             marker_color=color2))\n",
        "    \n",
        "    model1_neurons = list(model1['max_activation_index'].unique())\n",
        "    model2_neurons = list(model2['max_activation_index'].unique())\n",
        "    shared_neurons = find_shared_neurons(model1_neurons, model2_neurons)\n",
        "    for neuron in shared_neurons:\n",
        "        if model1_tokens_dict[neuron] > model2_tokens_dict[neuron]:\n",
        "            color_ = data_dict['visualize']['viz_colors']['length_reduced']\n",
        "        elif model1_tokens_dict[neuron] == model2_tokens_dict[neuron]:\n",
        "            color_ = 'black'\n",
        "        else:\n",
        "            color_ = data_dict['visualize']['viz_colors']['length_increased']\n",
        "        x_,y_ = [model1_tokens_dict[neuron],model2_tokens_dict[neuron]],[y_axis_label_model1 ,y_axis_label_model2]\n",
        "        fig.add_trace(go.Scatter(x= x_, y=y_, mode='lines', marker_color=color_, name=\" \"))\n",
        "\n",
        "    title_text = \"Length of \" + str(len(shared_neurons)) + \" alive neurons\"\n",
        "    fig.update_layout(showlegend=False, title_text=title_text, xaxis_type=\"log\", \n",
        "                      xaxis_title=\"Log number of tokens activated\")\n",
        "    \n",
        "    plotly.offline.plot(fig, filename = filename, auto_open=False)\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "dk2PPhGAOzS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def length_shift_pos_plot(model1, model2, modelname1, modelname2, color1, color2, y_axis_label_model1, \n",
        "                            y_axis_label_model2, data_dict, filename):\n",
        "    \"\"\"\n",
        "    :param model1:data from trained model 1(dtype:dataframe)\n",
        "    :param model2:data from trained model 2(dtype:dataframe)\n",
        "    :param color1:color for model 1(dtype:str)\n",
        "    :param color2:color for model 2(dtype:str)\n",
        "    :param modelname1:model1 label(dtype:str)\n",
        "    :param modelname2:model2 label(dtype:str)\n",
        "    :param y_axis_label_model1:Y axis label annotation for model1(dtype:str)\n",
        "    :param y_axis_label_model2:Y axis label annotation for model2(dtype:str)\n",
        "    :param data_dict: dictionary containing input instructions(dtype:dict)\n",
        "    :param filename: pickled file name and directory to store the results\n",
        "    \"\"\"\n",
        "        \n",
        "    fig = go.Figure()\n",
        "    # Collecting number of pos in each neurons for both the model\n",
        "    model1_pos_dict, model2_pos_dict = ({} for i in range(2))\n",
        "    for neuron in list(model1['max_activation_index'].unique()):\n",
        "        model1_data = model1[model1['max_activation_index'] == neuron]\n",
        "        model1_pos_dict[neuron] = model1_data['POS'].nunique()\n",
        "    for neuron in list(model2['max_activation_index'].unique()):\n",
        "        model2_data = model2[model2['max_activation_index'] == neuron]\n",
        "        model2_pos_dict[neuron] = model2_data['POS'].nunique()\n",
        "\n",
        "    model1_token_list, model1_y_list = ([] for i in range(2))\n",
        "    model2_token_list, model2_y_list = ([] for i in range(2))\n",
        "    # plotting scatter plot\n",
        "    for neuron in range(data_dict['models']['pretrained_lm']['nhid']):\n",
        "        if neuron in list(model1['max_activation_index'].unique()):\n",
        "            model1_token_list.append(model1_pos_dict[neuron])\n",
        "            model1_y_list.append(y_axis_label_model1)\n",
        "            \n",
        "        if neuron in list(model2['max_activation_index'].unique()):\n",
        "            model2_token_list.append(model2_pos_dict[neuron])\n",
        "            model2_y_list.append(y_axis_label_model2)\n",
        "    \n",
        "    fig.add_trace(go.Scatter(x=model1_token_list,y= model1_y_list, mode='markers', name=modelname1 ,\n",
        "                             marker_color=color1))\n",
        "    fig.add_trace(go.Scatter(x= model2_token_list, y=model2_y_list, mode='markers', name=modelname2,\n",
        "                             marker_color=color2))\n",
        "    \n",
        "    model1_neurons = list(model1['max_activation_index'].unique())\n",
        "    model2_neurons = list(model2['max_activation_index'].unique())\n",
        "    shared_neurons = find_shared_neurons(model1_neurons, model2_neurons)\n",
        "    for neuron in shared_neurons:\n",
        "        if model1_pos_dict[neuron] > model2_pos_dict[neuron]:\n",
        "            color_ = data_dict['visualize']['viz_colors']['length_reduced']\n",
        "        elif model1_pos_dict[neuron] == model2_pos_dict[neuron]:\n",
        "            color_ = 'black'\n",
        "        else:\n",
        "            color_ = data_dict['visualize']['viz_colors']['length_increased']\n",
        "        x_,y_ = [model1_pos_dict[neuron],model2_pos_dict[neuron]],[y_axis_label_model1 ,y_axis_label_model2]\n",
        "        fig.add_trace(go.Scatter(x= x_, y=y_, mode='lines', marker_color=color_, name=\" \"))\n",
        "\n",
        "    title_text = \"Length of \" + str(len(shared_neurons)) + \" alive neurons\"\n",
        "    fig.update_layout(showlegend=False, title_text=title_text, \n",
        "                      xaxis_title=\"number of POS activated\")\n",
        "    \n",
        "    plotly.offline.plot(fig, filename = filename, auto_open=False)\n",
        "    fig.show()\n"
      ],
      "metadata": {
        "id": "dl6PEhqJO6Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_top_pos_from_data(df):\n",
        "    \"\"\"\n",
        "    :param df: dataframe(dtype:pandas dataframe)\n",
        "    :returns: a dict with the top three pos tags associated with a token in the entire dataset.\n",
        "    \"\"\"\n",
        "    counter_dict = {}\n",
        "    unique_tokens = df['inputs'].unique()\n",
        "    for token in unique_tokens:\n",
        "        temp = df[df['inputs']==token]\n",
        "        temp_pos = list(temp['POS'])\n",
        "        temp_pos = [tag.strip() for tag in temp_pos]\n",
        "        tags = Counter(temp_pos)\n",
        "        most_common_tags = tags.most_common(3)\n",
        "        most_common_tags = [tags[0] for tags in most_common_tags]\n",
        "        counter_dict[token] = most_common_tags\n",
        "    return counter_dict\n",
        "\n",
        "\n",
        "\n",
        "def plot_top_10_hellinger_neurons(hellinger_stats, model1_data, model2_data, color1, color2, modelname1, modelname2, \n",
        "                                  data_dict, foldername, n_tokens=0, process_data_flag=False):\n",
        "    \"\"\"\n",
        "    :param hellinger_stats: path to the savd file for the hellinger statistics from calculate_hellinger_distance function \n",
        "    :param model1_data:data from trained model 1(dtype:dataframe)\n",
        "    :param model2_data:data from trained model 2(dtype:dataframe)\n",
        "    :param color1:color for model 1(dtype:str)\n",
        "    :param color2:color for model 2(dtype:str)\n",
        "    :param modelname1:model1 label(dtype:str)\n",
        "    :param modelname2:model2 label(dtype:str)\n",
        "    :param data_dict: dictionary containing input instructions(dtype:dict)\n",
        "    :param foldername: pickled file name and directory to store the results\n",
        "    :param n_tokens: number of tokens you want to plot(dtype:int)\n",
        "    :param process_data_flag: True if the pickle files need to be generated, False if you want to load the pickle \n",
        "                              files.\n",
        "    :Description: Generates the plot for the top 10 neurons with highest hellinger distances in hellinger_stats\n",
        "    \"\"\"\n",
        "    # removing the whitespaces\n",
        "    model1_data['POS'] = model1_data['POS'].apply(lambda x:x.replace(\" \",\"\"))\n",
        "    model2_data['POS'] = model2_data['POS'].apply(lambda x:x.replace(\" \",\"\"))\n",
        "    \n",
        "    # Getting all the POS tags activated\n",
        "    model1_pos = list(model1_data['POS'].unique())\n",
        "    model1_pos = list(model2_data['POS'].unique())\n",
        "    all_pos = set(model1_pos + model1_pos)\n",
        "    # all_pos = [pos.strip() for pos in all_pos]\n",
        "    \n",
        "    # loading the Hellinger distance dictionary\n",
        "    with open(hellinger_stats, 'rb') as handle:\n",
        "        hellinger_dict = pickle.load(handle)\n",
        "        \n",
        "    top_10_neurons = heapq.nlargest(10, hellinger_dict, key=hellinger_dict.get)\n",
        "    for neuron in top_10_neurons:\n",
        "        path = os.path.join(data_dict[\"visualize\"][\"plot_directory\"],foldername,\"top_10\",str(neuron))\n",
        "        \n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "        \n",
        "        model1_data_temp = model1_data[model1_data['max_activation_index']==neuron]\n",
        "        model2_data_temp = model2_data[model2_data['max_activation_index']==neuron]\n",
        "                \n",
        "        # Getting the pos stats from all the dictionaries\n",
        "        model1_pos_dict = dict(Counter(model1_data_temp['POS']))\n",
        "        model2_pos_dict = dict(Counter(model2_data_temp['POS']))\n",
        "        # Creating dataframe from the dictionaries\n",
        "        model1_pos = pd.DataFrame.from_dict(model1_pos_dict, orient='index', columns=[modelname1])\n",
        "        model2_pos = pd.DataFrame.from_dict(model2_pos_dict, orient='index', columns=[modelname2])\n",
        "        # Normalizing the statistics\n",
        "        model1_pos[modelname1] = model1_pos[modelname1].apply(lambda x: x/model1_pos[modelname1].sum())\n",
        "        model2_pos[modelname2] = model2_pos[modelname2].apply(lambda x: x/model2_pos[modelname2].sum())\n",
        "        # Merging dataframe\n",
        "        data = [model1_pos[modelname1], model2_pos[modelname2]]\n",
        "        df = pd.concat(data,axis=1)\n",
        "        # Again converting the dataframe to dictionary for further computations.\n",
        "        all_pos_stats = df.to_dict()\n",
        "        \n",
        "        # Getting all the pos stats into a dictionary\n",
        "        for viz_data in all_pos_stats.keys():\n",
        "            for tags in all_pos:\n",
        "                if tags not in all_pos_stats[viz_data].keys():\n",
        "                    all_pos_stats[viz_data][tags] = None\n",
        "            \n",
        "        # Converting pos stats to a dataframe\n",
        "        # all_pos_stats = pd.DataFrame.from_dict(all_pos_stats)\n",
        "        \n",
        "        if process_data_flag == True:\n",
        "            # Getting the data.\n",
        "            model1_neurondata = model1_data[model1_data['max_activation_index']==neuron]\n",
        "            model1_neurondata['POS'] = model1_neurondata['POS'].apply(lambda x: x.strip())\n",
        "            model2_neurondata = model2_data[model2_data['max_activation_index']==neuron]\n",
        "            model2_neurondata['POS'] = model2_neurondata['POS'].apply(lambda x: x.strip())\n",
        "            \n",
        "            # Converting the other pos tags to the top three ones\n",
        "            model1_top_pos = choose_top_pos_from_data(model1_neurondata)\n",
        "            model2_top_pos = choose_top_pos_from_data(model2_neurondata)\n",
        "            \n",
        "            model1_tokens = list(model1_neurondata['inputs'])\n",
        "            model1_pos = list(model1_neurondata['POS'])\n",
        "            model2_tokens = list(model2_neurondata['inputs'])\n",
        "            model2_pos = list(model2_neurondata['POS'])\n",
        "\n",
        "            for index, pos in enumerate(model1_pos):\n",
        "                if pos not in model1_top_pos[model1_tokens[index]]:\n",
        "                    model1_pos[index] = model1_top_pos[model1_tokens[index]][0]\n",
        "            for index, pos in enumerate(model2_pos):\n",
        "                if pos not in model2_top_pos[model2_tokens[index]]:\n",
        "                    model2_pos[index] = model2_top_pos[model2_tokens[index]][0]\n",
        "                    \n",
        "            model1_neurondata['POS'] = model1_pos\n",
        "            model2_neurondata['POS'] = model2_pos\n",
        "            \n",
        "            # Getting all the unique tokens\n",
        "            model1_unique_tokens = model1_neurondata[\"inputs\"].unique()\n",
        "            model2_unique_tokens = model2_neurondata[\"inputs\"].unique()\n",
        "            \n",
        "            model1_dict,model2_dict = ({} for i in range(2))\n",
        "            \n",
        "            # Generating model1 visualization\n",
        "            # Getting mean for all the unique tokens\n",
        "            for tokens in model1_unique_tokens:\n",
        "                temp_df = model1_neurondata[model1_neurondata[\"inputs\"] == tokens]\n",
        "                pos = list(temp_df[\"POS\"].unique())\n",
        "                activation_temp = []\n",
        "                for unique_pos in pos:\n",
        "                    activation_temp.append(temp_df[temp_df['POS']==unique_pos][\"max_activations\"].mean())\n",
        "                model1_dict[tokens] = {\"POS\":pos, \"activation\":activation_temp}\n",
        "            \n",
        "            # Getting the top 20 activation tokens\n",
        "            model1_top_20 = {}\n",
        "            temp_activations, temp_tokens = ([] for i in range(2))\n",
        "            for key, value in model1_dict.items():\n",
        "                for index in range(len(value['POS'])):\n",
        "                    temp_tokens.append(key)\n",
        "                    temp_activations.append(value['activation'][index])      \n",
        "            model1_top_20_activation_index = sorted(range(len(temp_activations)), key=lambda x: temp_activations[x])[-n_tokens:]\n",
        "            for indexes in model1_top_20_activation_index:\n",
        "                model1_top_20[temp_tokens[indexes]] = model1_dict[temp_tokens[indexes]]\n",
        "            \n",
        "            # Flipping the dictionary to get it in the order of {pos-tags:list(tuple(token,mean_activations))}\n",
        "            model1_token_dict = defaultdict(list)\n",
        "            for token,stats in model1_top_20.items():\n",
        "                for index,value in enumerate(stats['POS']):\n",
        "                    model1_token_dict[stats['POS'][index]].append((token,stats['activation'][index]))\n",
        "            \n",
        "            # Adding the null features for the tags not present\n",
        "            for tags in all_pos:\n",
        "                if tags not in model1_token_dict.keys():\n",
        "                    model1_token_dict[tags].append((' ',0.0))\n",
        "\n",
        "            # Sorting dict on the basis of the names\n",
        "            sorted_model1_dict = {}\n",
        "            for key in sorted(model1_token_dict.keys()):\n",
        "                sorted_model1_dict[key] = model1_token_dict[key]\n",
        "                \n",
        "            with open(os.path.join(path,'model1_data.pickle'), 'wb') as handle:\n",
        "                pickle.dump(sorted_model1_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                \n",
        "            # Generating model2 visualization\n",
        "            # Getting mean for all the unique tokens\n",
        "            for tokens in model2_unique_tokens:\n",
        "                temp_df = model2_neurondata[model2_neurondata[\"inputs\"] == tokens]\n",
        "                pos = list(temp_df[\"POS\"].unique())\n",
        "                activation_temp = []\n",
        "                for unique_pos in pos:\n",
        "                    activation_temp.append(temp_df[temp_df['POS']==unique_pos][\"max_activations\"].mean())\n",
        "                model2_dict[tokens] = {\"POS\":pos, \"activation\":activation_temp}\n",
        "            \n",
        "            # Getting the top 20 activation tokens\n",
        "            model2_top_20 = {}\n",
        "            temp_activations, temp_tokens = ([] for i in range(2))\n",
        "            for key, value in model2_dict.items():\n",
        "                for index in range(len(value['POS'])):\n",
        "                    temp_tokens.append(key)\n",
        "                    temp_activations.append(value['activation'][index])      \n",
        "            model2_top_20_activation_index = sorted(range(len(temp_activations)), key=lambda x: temp_activations[x])[-n_tokens:]\n",
        "            for indexes in model2_top_20_activation_index:\n",
        "                model2_top_20[temp_tokens[indexes]] = model2_dict[temp_tokens[indexes]]\n",
        "            \n",
        "            # Flipping the dictionary to get it in the order of {pos-tags:list(tuple(token,mean_activations))}\n",
        "            model2_token_dict = defaultdict(list)\n",
        "            for token,stats in model2_top_20.items():\n",
        "                for index,value in enumerate(stats['POS']):\n",
        "                    model2_token_dict[stats['POS'][index]].append((token,stats['activation'][index]))\n",
        "            \n",
        "            # Adding the null features for the tags not present\n",
        "            for tags in all_pos:\n",
        "                if tags not in model2_token_dict.keys():\n",
        "                    model2_token_dict[tags].append((' ',0.0))\n",
        "\n",
        "            # Sorting dict on the basis of the names\n",
        "            sorted_model2_dict = {}\n",
        "            for key in sorted(model2_token_dict.keys()):\n",
        "                sorted_model2_dict[key] = model2_token_dict[key]\n",
        "                \n",
        "            with open(os.path.join(path,'model2_data.pickle'), 'wb') as handle:\n",
        "                pickle.dump(sorted_model2_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                \n",
        "        else:\n",
        "            # loading the dictionary\n",
        "            with open(os.path.join(path,'model1_data.pickle'), 'rb') as handle:\n",
        "                sorted_model1_dict = pickle.load(handle)\n",
        "            with open(os.path.join(path,'model2_data.pickle'), 'rb') as handle:\n",
        "                sorted_model2_dict = pickle.load(handle)\n",
        "                \n",
        "        fig = go.Figure()\n",
        "        # Plotting the bar plot\n",
        "        fig.add_trace(go.Bar(x=list(all_pos_stats[modelname1].keys()), y=list(all_pos_stats[modelname1].values()), \n",
        "                             name=modelname1, marker_color=color1, opacity=0.6))\n",
        "        fig.add_trace(go.Bar(x=list(all_pos_stats[modelname2].keys()), y=list(all_pos_stats[modelname2].values()), \n",
        "                             name=modelname2, marker_color=color2, opacity=0.6))\n",
        "\n",
        "        # Plotting the tokens on the bar plot\n",
        "        pos_model1 = list(sorted_model1_dict.keys())\n",
        "        values_model1 = list(sorted_model1_dict.values())\n",
        "\n",
        "        pos_model2 = list(sorted_model2_dict.keys())\n",
        "        values_model2 = list(sorted_model2_dict.values())\n",
        "        model1_value = [[(value[0],np.nan) if value[1]==0.0 else (value[0],value[1]) for value in pairs] for pairs in values_model1]\n",
        "        model2_value = [[(value[0],np.nan) if value[1]==0.0 else (value[0],value[1]) for value in pairs] for pairs in values_model2]\n",
        "\n",
        "        model1_token = [[value[0] for value in pairs] for pairs in model1_value]\n",
        "        model1_activations = [[value[1] for value in pairs] for pairs in model1_value]\n",
        "\n",
        "        model2_token = [[value[0] for value in pairs] for pairs in model2_value]\n",
        "        model2_activations = [[value[1] for value in pairs] for pairs in model2_value]\n",
        "\n",
        "        pos_model1_list, activation_model1_list, token_model1_list = ([] for i in range(3))\n",
        "        for index in range(len(pos_model1)):\n",
        "            for activation_list_index, activation in enumerate(model1_activations[index]):\n",
        "                if activation >= 0.0:\n",
        "                \tpos_model1_list.append(pos_model1[index])\n",
        "                \tactivation_model1_list.append(activation)\n",
        "                \ttoken_model1_list.append(model1_token[index][activation_list_index])\n",
        "        fig.add_trace(go.Scatter(x=pos_model1_list, y=activation_model1_list, text=token_model1_list, \n",
        "                                 mode='markers+text', marker_color=color1, name=modelname1, \n",
        "                                 textfont={'color':color1}))\n",
        "\n",
        "        pos_model2_list, activation_model2_list, token_model2_list = ([] for i in range(3))\n",
        "        for index in range(len(pos_model2)):\n",
        "            for activation_list_index, activation in enumerate(model2_activations[index]):\n",
        "            \tif activation >= 0.0:\n",
        "                \tpos_model2_list.append(pos_model2[index])\n",
        "                \tactivation_model2_list.append(activation)\n",
        "                \ttoken_model2_list.append(model2_token[index][activation_list_index])\n",
        "        fig.add_trace(go.Scatter(x=pos_model2_list, y=activation_model2_list, text=token_model2_list, \n",
        "                                 mode='markers+text', marker_color=color2, name=modelname2, \n",
        "                                 textfont={'color':color2}))\n",
        "        \n",
        "        fig.update_layout(title_text='Hellinger plot for ' + str(neuron) + \"-neuron\" ,\n",
        "                    xaxis_title=\"POS-tags\",\n",
        "                    yaxis_title=\"Activation\",\n",
        "                    xaxis = go.XAxis(showticklabels=True),\n",
        "                    yaxis = go.YAxis(showticklabels=True)\n",
        "                    )\n",
        "        \n",
        "        plotly.offline.plot(fig, filename = os.path.join(path,str(neuron)+\".pdf\"), auto_open=False)\n",
        "        fig.show()\n",
        "        "
      ],
      "metadata": {
        "id": "42bwETBDO6DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_least_10_hellinger_neurons(hellinger_stats, model1_data, model2_data, color1, color2, modelname1, modelname2, \n",
        "                                  data_dict, foldername, n_tokens=0, process_data_flag=False):\n",
        "    \"\"\"\n",
        "    :param hellinger_stats: path to the savd file for the hellinger statistics from calculate_hellinger_distance function \n",
        "    :param model1_data:data from trained model 1(dtype:dataframe)\n",
        "    :param model2_data:data from trained model 2(dtype:dataframe)\n",
        "    :param color1:color for model 1(dtype:str)\n",
        "    :param color2:color for model 2(dtype:str)\n",
        "    :param modelname1:model1 label(dtype:str)\n",
        "    :param modelname2:model2 label(dtype:str)\n",
        "    :param data_dict: dictionary containing input instructions(dtype:dict)\n",
        "    :param foldername: pickled file name and directory to store the results\n",
        "    :param n_tokens: number of tokens you want to plot(dtype:int)\n",
        "    :param process_data_flag: True if the pickle files need to be generated, False if you want to load the pickle \n",
        "                              files.\n",
        "    :Description: Generates the plot for the least 10 neurons with highest hellinger distances in hellinger_stats\n",
        "    \"\"\"\n",
        "    # removing the whitespaces\n",
        "    model1_data['POS'] = model1_data['POS'].apply(lambda x:x.replace(\" \",\"\"))\n",
        "    model2_data['POS'] = model2_data['POS'].apply(lambda x:x.replace(\" \",\"\"))\n",
        "    \n",
        "    # Getting all the POS tags activated\n",
        "    model1_pos = list(model1_data['POS'].unique())\n",
        "    model1_pos = list(model2_data['POS'].unique())\n",
        "    all_pos = set(model1_pos + model1_pos)\n",
        "    # all_pos = [pos.strip() for pos in all_pos]\n",
        "    \n",
        "    # loading the Hellinger distance dictionary\n",
        "    with open(hellinger_stats, 'rb') as handle:\n",
        "        hellinger_dict = pickle.load(handle)\n",
        "        \n",
        "    least_10_neurons = heapq.nsmallest(10, hellinger_dict, key=hellinger_dict.get)\n",
        "    for neuron in least_10_neurons:\n",
        "        path = os.path.join(data_dict[\"visualize\"][\"plot_directory\"],foldername,\"least_10\",str(neuron))\n",
        "        \n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "        \n",
        "        model1_data_temp = model1_data[model1_data['max_activation_index']==neuron]\n",
        "        model2_data_temp = model2_data[model2_data['max_activation_index']==neuron]\n",
        "                \n",
        "        # Getting the pos stats from all the dictionaries\n",
        "        model1_pos_dict = dict(Counter(model1_data_temp['POS']))\n",
        "        model2_pos_dict = dict(Counter(model2_data_temp['POS']))\n",
        "        # Creating dataframe from the dictionaries\n",
        "        model1_pos = pd.DataFrame.from_dict(model1_pos_dict, orient='index', columns=[modelname1])\n",
        "        model2_pos = pd.DataFrame.from_dict(model2_pos_dict, orient='index', columns=[modelname2])\n",
        "        # Normalizing the statistics\n",
        "        model1_pos[modelname1] = model1_pos[modelname1].apply(lambda x: x/model1_pos[modelname1].sum())\n",
        "        model2_pos[modelname2] = model2_pos[modelname2].apply(lambda x: x/model2_pos[modelname2].sum())\n",
        "        # Merging dataframe\n",
        "        data = [model1_pos[modelname1], model2_pos[modelname2]]\n",
        "        df = pd.concat(data,axis=1)\n",
        "        # Again converting the dataframe to dictionary for further computations.\n",
        "        all_pos_stats = df.to_dict()\n",
        "        \n",
        "        # Getting all the pos stats into a dictionary\n",
        "        for viz_data in all_pos_stats.keys():\n",
        "            for tags in all_pos:\n",
        "                if tags not in all_pos_stats[viz_data].keys():\n",
        "                    all_pos_stats[viz_data][tags] = None\n",
        "            \n",
        "        # Converting pos stats to a dataframe\n",
        "        # all_pos_stats = pd.DataFrame.from_dict(all_pos_stats)\n",
        "        \n",
        "        if process_data_flag == True:\n",
        "            # Getting the data.\n",
        "            model1_neurondata = model1_data[model1_data['max_activation_index']==neuron]\n",
        "            model1_neurondata['POS'] = model1_neurondata['POS'].apply(lambda x: x.strip())\n",
        "            model2_neurondata = model2_data[model2_data['max_activation_index']==neuron]\n",
        "            model2_neurondata['POS'] = model2_neurondata['POS'].apply(lambda x: x.strip())\n",
        "            \n",
        "            # Converting the other pos tags to the least three ones\n",
        "            model1_least_pos = choose_top_pos_from_data(model1_neurondata)\n",
        "            model2_least_pos = choose_top_pos_from_data(model2_neurondata)\n",
        "            \n",
        "            model1_tokens = list(model1_neurondata['inputs'])\n",
        "            model1_pos = list(model1_neurondata['POS'])\n",
        "            model2_tokens = list(model2_neurondata['inputs'])\n",
        "            model2_pos = list(model2_neurondata['POS'])\n",
        "\n",
        "            for index, pos in enumerate(model1_pos):\n",
        "                if pos not in model1_least_pos[model1_tokens[index]]:\n",
        "                    model1_pos[index] = model1_least_pos[model1_tokens[index]][0]\n",
        "            for index, pos in enumerate(model2_pos):\n",
        "                if pos not in model2_least_pos[model2_tokens[index]]:\n",
        "                    model2_pos[index] = model2_least_pos[model2_tokens[index]][0]\n",
        "                    \n",
        "            model1_neurondata['POS'] = model1_pos\n",
        "            model2_neurondata['POS'] = model2_pos\n",
        "            \n",
        "            # Getting all the unique tokens\n",
        "            model1_unique_tokens = model1_neurondata[\"inputs\"].unique()\n",
        "            model2_unique_tokens = model2_neurondata[\"inputs\"].unique()\n",
        "            \n",
        "            model1_dict,model2_dict = ({} for i in range(2))\n",
        "            \n",
        "            # Generating model1 visualization\n",
        "            # Getting mean for all the unique tokens\n",
        "            for tokens in model1_unique_tokens:\n",
        "                temp_df = model1_neurondata[model1_neurondata[\"inputs\"] == tokens]\n",
        "                pos = list(temp_df[\"POS\"].unique())\n",
        "                activation_temp = []\n",
        "                for unique_pos in pos:\n",
        "                    activation_temp.append(temp_df[temp_df['POS']==unique_pos][\"max_activations\"].mean())\n",
        "                model1_dict[tokens] = {\"POS\":pos, \"activation\":activation_temp}\n",
        "            \n",
        "            # Getting the least 20 activation tokens\n",
        "            model1_least_20 = {}\n",
        "            temp_activations, temp_tokens = ([] for i in range(2))\n",
        "            for key, value in model1_dict.items():\n",
        "                for index in range(len(value['POS'])):\n",
        "                    temp_tokens.append(key)\n",
        "                    temp_activations.append(value['activation'][index])      \n",
        "            model1_least_20_activation_index = sorted(range(len(temp_activations)), key=lambda x: temp_activations[x])[-n_tokens:]\n",
        "            for indexes in model1_least_20_activation_index:\n",
        "                model1_least_20[temp_tokens[indexes]] = model1_dict[temp_tokens[indexes]]\n",
        "            \n",
        "            # Flipping the dictionary to get it in the order of {pos-tags:list(tuple(token,mean_activations))}\n",
        "            model1_token_dict = defaultdict(list)\n",
        "            for token,stats in model1_least_20.items():\n",
        "                for index,value in enumerate(stats['POS']):\n",
        "                    model1_token_dict[stats['POS'][index]].append((token,stats['activation'][index]))\n",
        "            \n",
        "            # Adding the null features for the tags not present\n",
        "            for tags in all_pos:\n",
        "                if tags not in model1_token_dict.keys():\n",
        "                    model1_token_dict[tags].append((' ',0.0))\n",
        "\n",
        "            # Sorting dict on the basis of the names\n",
        "            sorted_model1_dict = {}\n",
        "            for key in sorted(model1_token_dict.keys()):\n",
        "                sorted_model1_dict[key] = model1_token_dict[key]\n",
        "                \n",
        "            with open(os.path.join(path,'model1_data.pickle'), 'wb') as handle:\n",
        "                pickle.dump(sorted_model1_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                \n",
        "            # Generating model2 visualization\n",
        "            # Getting mean for all the unique tokens\n",
        "            for tokens in model2_unique_tokens:\n",
        "                temp_df = model2_neurondata[model2_neurondata[\"inputs\"] == tokens]\n",
        "                pos = list(temp_df[\"POS\"].unique())\n",
        "                activation_temp = []\n",
        "                for unique_pos in pos:\n",
        "                    activation_temp.append(temp_df[temp_df['POS']==unique_pos][\"max_activations\"].mean())\n",
        "                model2_dict[tokens] = {\"POS\":pos, \"activation\":activation_temp}\n",
        "            \n",
        "            # Getting the least 20 activation tokens\n",
        "            model2_least_20 = {}\n",
        "            temp_activations, temp_tokens = ([] for i in range(2))\n",
        "            for key, value in model2_dict.items():\n",
        "                for index in range(len(value['POS'])):\n",
        "                    temp_tokens.append(key)\n",
        "                    temp_activations.append(value['activation'][index])      \n",
        "            model2_least_20_activation_index = sorted(range(len(temp_activations)), key=lambda x: temp_activations[x])[-n_tokens:]\n",
        "            for indexes in model2_least_20_activation_index:\n",
        "                model2_least_20[temp_tokens[indexes]] = model2_dict[temp_tokens[indexes]]\n",
        "            \n",
        "            # Flipping the dictionary to get it in the order of {pos-tags:list(tuple(token,mean_activations))}\n",
        "            model2_token_dict = defaultdict(list)\n",
        "            for token,stats in model2_least_20.items():\n",
        "                for index,value in enumerate(stats['POS']):\n",
        "                    model2_token_dict[stats['POS'][index]].append((token,stats['activation'][index]))\n",
        "            \n",
        "            # Adding the null features for the tags not present\n",
        "            for tags in all_pos:\n",
        "                if tags not in model2_token_dict.keys():\n",
        "                    model2_token_dict[tags].append((' ',0.0))\n",
        "\n",
        "            # Sorting dict on the basis of the names\n",
        "            sorted_model2_dict = {}\n",
        "            for key in sorted(model2_token_dict.keys()):\n",
        "                sorted_model2_dict[key] = model2_token_dict[key]\n",
        "                \n",
        "            with open(os.path.join(path,'model2_data.pickle'), 'wb') as handle:\n",
        "                pickle.dump(sorted_model2_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                \n",
        "        else:\n",
        "            # loading the dictionary\n",
        "            with open(os.path.join(path,'model1_data.pickle'), 'rb') as handle:\n",
        "                sorted_model1_dict = pickle.load(handle)\n",
        "            with open(os.path.join(path,'model2_data.pickle'), 'rb') as handle:\n",
        "                sorted_model2_dict = pickle.load(handle)\n",
        "                \n",
        "        fig = go.Figure()\n",
        "        # Plotting the bar plot\n",
        "        fig.add_trace(go.Bar(x=list(all_pos_stats[modelname1].keys()), y=list(all_pos_stats[modelname1].values()), \n",
        "                             name=modelname1, marker_color=color1, opacity=0.6))\n",
        "        fig.add_trace(go.Bar(x=list(all_pos_stats[modelname2].keys()), y=list(all_pos_stats[modelname2].values()), \n",
        "                             name=modelname2, marker_color=color2, opacity=0.6))\n",
        "\n",
        "        # Plotting the tokens on the bar plot\n",
        "        pos_model1 = list(sorted_model1_dict.keys())\n",
        "        values_model1 = list(sorted_model1_dict.values())\n",
        "\n",
        "        pos_model2 = list(sorted_model2_dict.keys())\n",
        "        values_model2 = list(sorted_model2_dict.values())\n",
        "        model1_value = [[(value[0],np.nan) if value[1]==0.0 else (value[0],value[1]) for value in pairs] for pairs in values_model1]\n",
        "        model2_value = [[(value[0],np.nan) if value[1]==0.0 else (value[0],value[1]) for value in pairs] for pairs in values_model2]\n",
        "\n",
        "        model1_token = [[value[0] for value in pairs] for pairs in model1_value]\n",
        "        model1_activations = [[value[1] for value in pairs] for pairs in model1_value]\n",
        "\n",
        "        model2_token = [[value[0] for value in pairs] for pairs in model2_value]\n",
        "        model2_activations = [[value[1] for value in pairs] for pairs in model2_value]\n",
        "\n",
        "        pos_model1_list, activation_model1_list, token_model1_list = ([] for i in range(3))\n",
        "        for index in range(len(pos_model1)):\n",
        "            for activation_list_index, activation in enumerate(model1_activations[index]):\n",
        "                pos_model1_list.append(pos_model1[index])\n",
        "                activation_model1_list.append(activation)\n",
        "                token_model1_list.append(model1_token[index][activation_list_index])\n",
        "        fig.add_trace(go.Scatter(x=pos_model1_list, y=activation_model1_list, text=token_model1_list, \n",
        "                                 mode='markers+text', marker_color=color1, name=modelname1, \n",
        "                                 textfont={'color':color1}))\n",
        "\n",
        "        pos_model2_list, activation_model2_list, token_model2_list = ([] for i in range(3))\n",
        "        for index in range(len(pos_model2)):\n",
        "            for activation_list_index, activation in enumerate(model2_activations[index]):\n",
        "                pos_model2_list.append(pos_model2[index])\n",
        "                activation_model2_list.append(activation)\n",
        "                token_model2_list.append(model2_token[index][activation_list_index])\n",
        "        fig.add_trace(go.Scatter(x=pos_model2_list, y=activation_model2_list, text=token_model2_list, \n",
        "                                 mode='markers+text', marker_color=color2, name=modelname2, \n",
        "                                 textfont={'color':color2}))\n",
        "        \n",
        "        fig.update_layout(title_text='Hellinger plot for ' + str(neuron) + \"-neuron\" ,\n",
        "                    xaxis_title=\"POS-tags\",\n",
        "                    yaxis_title=\"Activation\",\n",
        "                    xaxis = go.XAxis(showticklabels=True),\n",
        "                    yaxis = go.YAxis(showticklabels=True)\n",
        "                    )\n",
        "        \n",
        "        plotly.offline.plot(fig, filename = os.path.join(path,str(neuron)+\".pdf\"), auto_open=False)\n",
        "        fig.show()"
      ],
      "metadata": {
        "id": "lA4R_B9fO6Fl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}