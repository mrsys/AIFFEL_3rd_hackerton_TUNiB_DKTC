{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "layers",
      "provenance": [],
      "authorship_tag": "ABX9TyOHKXesS5OXnV+8ADFSehk8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sda96/AIFFEL_3rd_hackerton_TUNiB_DKTC/blob/main/notebook/SeHyun/Custom%20Libraries/layers.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgqdJFmyQRKy"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMLM(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        super(LSTMLM, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.lstm = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError:\n",
        "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
        "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
        "            self.lstm = nn.LSTM(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        # Enabling weight tying\n",
        "        # Whenever the dimensions of inputs and ouputs are same, weights can be shared in between to reduce the number\n",
        "        # of training parameters and improve the performance.\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        # Uniformly initializing weights between two values\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        encoded_output_from_embedding_layer, hidden = self.lstm(emb, hidden)\n",
        "        output = self.drop(encoded_output_from_embedding_layer)\n",
        "        decoded_output_from_linear_layer = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return encoded_output_from_embedding_layer, decoded_output_from_linear_layer.view(output.size(0), output.size(1), decoded_output_from_linear_layer.size(1)), hidden\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        # Initializes the hidden and cell state\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros(self.nlayers, batch_size, self.nhid),\n",
        "                    weight.new_zeros(self.nlayers, batch_size, self.nhid))\n",
        "        else:\n",
        "            return weight.new_zeros(self.nlayers, batch_size, self.nhid)\n"
      ],
      "metadata": {
        "id": "3XDirjlLQgDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    1) mode for the model can either be train or generate\n",
        "    2) When the model is in training mode, it takes the entire paragraph and pass it through nn.LSTM to make the \n",
        "       training efficient.\n",
        "    3) When the model is in generation mode, it takes one token at time from the paragraphs and update the hidden\n",
        "       activation to get intermediate data.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, pre_trained_model, wiki_idx_to_word, activations_to_prune=None):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # The label size is been set to one for IMDB movie review dataset so to use BCEWithLogitsLoss.\n",
        "        self.linear_layer = nn.Linear(hidden_dim, 1)\n",
        "        self.pretrained_model = pre_trained_model\n",
        "        self.wiki_idx_to_word = wiki_idx_to_word\n",
        "\n",
        "        # Neurons to prune\n",
        "        self.prunned_activations = activations_to_prune\n",
        "        \n",
        "    def init_hidden(self):\n",
        "        return (torch.tensor(torch.zeros(1, 1, self.hidden_dim)),\n",
        "                torch.tensor(torch.zeros(1, 1, self.hidden_dim)))\n",
        "    \n",
        "    def forward(self, paragraphs, eval_flag=False):\n",
        "        # Getting  modified hidden state from the pretrained model  \n",
        "        if eval_flag:\n",
        "            # To disable batchnorm and dropout layers in evaluation and test mode\n",
        "            self.pretrained_model.eval()\n",
        "        else:\n",
        "            # To stop the gradient flow back during back-propagation\n",
        "            self.pretrained_model.train()\n",
        "        \n",
        "        # Freezing the embedding layer of the pretrained model\n",
        "        self.pretrained_model.encoder.weight.requires_grad = False\n",
        "        value_dict = {}\n",
        "        \n",
        "        # The hidden state of the lSTM unit is supposed to be  (num_layers * num_directions, batch, hidden_size)\n",
        "        hidden = self.init_hidden()\n",
        "        # The input to the LSTM unit is supposed to be (seq_len, batch, input_size)\n",
        "        # Here the seq_length is set to the length of paragraphs\n",
        "        # batch_size is set to one.\n",
        "        paragraphs = paragraphs.resize_((paragraphs.size(0),1))\n",
        "        # Getting the input hidden activations\n",
        "        encoded_outputs, decoded_outputs, hidden = self.pretrained_model(paragraphs, hidden)\n",
        "        # Getting the output hidden activation(i.e the hidden state is a tuple with (input_hidden,output_hidden))\n",
        "        # hidden[0] indicates the output hidden activation and hidden[1] indicates insput hidden activation\n",
        "        hidden_out = hidden[0]\n",
        "        \n",
        "        if self.prunned_activations is not None:\n",
        "            # Performing pruning\n",
        "            hidden_out_temp = hidden[0].view(-1)\n",
        "            hidden_in_temp = hidden[1].view(-1)\n",
        "\n",
        "            for activation in self.prunned_activations:\n",
        "                hidden_out_temp[activation] = 0.0000\n",
        "                # hidden_in_temp[activation] = 0.0000\n",
        "            hidden = (hidden_out_temp.view(1,1,1500),hidden_in_temp.view(1,1,1500))\n",
        "        \n",
        "        value_dict[\"inputs\"] = paragraphs.data.numpy()\n",
        "        activations = encoded_outputs.detach()\n",
        "        max_activations = [float(act.squeeze_().max()) for act in activations]\n",
        "        max_activation_index = [int(act.squeeze_().argmax()) for act in activations]\n",
        "                \n",
        "        value_dict[\"activations\"] = activations\n",
        "        value_dict[\"max_activations\"] = max_activations\n",
        "        value_dict[\"max_activation_index\"] = max_activation_index\n",
        "\n",
        "        # hidden state is a 3D tensor of (seq_length, batch_size, hidden_activation) since we are only concerned\n",
        "        # with the last hidden activation we take [-1] index and send it through the linear layer\n",
        "        y_logits  = self.linear_layer(hidden_out[0][-1])\n",
        "        # BCEwithlogitsloss already has a sigmoid layer in it and the line below is for BCEwithlogitsloss.\n",
        "        return y_logits, value_dict\n"
      ],
      "metadata": {
        "id": "l9HvzssWQgGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMPoolingClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    1) mode for the model can either be train or generate\n",
        "    2) When the model is in training mode, it takes the entire paragraph and pass it through nn.LSTM to make the \n",
        "       training efficient.\n",
        "    3) When the model is in generation mode, it takes one token at time from the paragraphs and update the hidden\n",
        "       activation to get intermediate data.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size, pre_trained_model, wiki_idx_to_word):\n",
        "        super(LSTMPoolingClassifier, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # The label size is been set to one for IMDB movie review dataset so to use BCEWithLogitsLoss.\n",
        "        self.linear_layer = nn.Linear(3*hidden_dim, 1)\n",
        "        self.pretrained_model = pre_trained_model\n",
        "        self.wiki_idx_to_word = wiki_idx_to_word\n",
        "            \n",
        "    def init_hidden(self):\n",
        "        return (torch.tensor(torch.zeros(1, 1, self.hidden_dim)),\n",
        "                torch.tensor(torch.zeros(1, 1, self.hidden_dim)))\n",
        "    \n",
        "    def forward(self, paragraphs, eval_flag=False):\n",
        "        # Getting  modified hidden state from the pretrained model  \n",
        "        if eval_flag:\n",
        "            # To disable batchnorm and dropout layers in evaluation and test mode\n",
        "            self.pretrained_model.eval()\n",
        "        else:\n",
        "            # To stop the gradient flow back during back-propagation\n",
        "            self.pretrained_model.train()\n",
        "        \n",
        "        # Freezing the embedding layer of the pretrained model\n",
        "        self.pretrained_model.encoder.weight.requires_grad = False\n",
        "        value_dict = {}\n",
        "        \n",
        "        # The hidden state of the lSTM unit is supposed to be  (num_layers * num_directions, batch, hidden_size)\n",
        "        hidden = self.init_hidden()\n",
        "        # The input to the LSTM unit is supposed to be (seq_len, batch, input_size)\n",
        "        # Here the seq_length is set to the length of paragraphs\n",
        "        # batch_size is set to one.\n",
        "        paragraphs = paragraphs.resize_((paragraphs.size(0),1))\n",
        "        # Getting the input hidden activations\n",
        "        encoded_outputs, decoded_outputs, hidden = self.pretrained_model(paragraphs, hidden)\n",
        "        # Getting the output hidden activation(i.e the hidden state is a tuple with (input_hidden,output_hidden))\n",
        "        # hidden[0] indicates the output hidden activation and hidden[1] indicates insput hidden activation\n",
        "        hidden_out = hidden[0]\n",
        "        # bringing hidden_out in the shape of (1,1500) from (1,1,1500)\n",
        "        hidden_out = hidden_out.squeeze(1)\n",
        "\n",
        "        average_hidden = torch.nn.functional.adaptive_avg_pool1d(encoded_outputs.permute(1,2,0), (1,)).view(1,-1)\n",
        "        max_hidden = torch.nn.functional.adaptive_max_pool1d(encoded_outputs.permute(1,2,0), (1,)).view(1,-1)\n",
        "        # Concatenating hidden activations with avg pooling and max pooling\n",
        "        cat_hidden = torch.cat([hidden_out,average_hidden,max_hidden], 1)\n",
        "        \n",
        "        value_dict[\"inputs\"] = paragraphs.data.numpy()\n",
        "        activations = encoded_outputs.detach()\n",
        "        max_activations = [float(act.squeeze_().max()) for act in activations]\n",
        "        max_activation_index = [int(act.squeeze_().argmax()) for act in activations]\n",
        "                \n",
        "        value_dict[\"activations\"] = activations\n",
        "        value_dict[\"max_activations\"] = max_activations\n",
        "        value_dict[\"max_activation_index\"] = max_activation_index\n",
        "\n",
        "        # hidden state is a 3D tensor of (seq_length, batch_size, hidden_activation) since we are only concerned\n",
        "        # with the last hidden activation we take [-1] index and send it through the linear layer\n",
        "        y_logits  = self.linear_layer(cat_hidden[-1])\n",
        "        # BCEwithlogitsloss already has a sigmoid layer in it and the line below is for BCEwithlogitsloss.\n",
        "        return y_logits, value_dict"
      ],
      "metadata": {
        "id": "BhY_YdE4QgIy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}